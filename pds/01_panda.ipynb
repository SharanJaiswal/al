{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a6570200391abd47b22bafa3ab36aba8327b6bca09374d4d4cb5f21cfca217ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pandas Datatype\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html\n",
    "## pandas.Series\n",
    "1. strictly homo and 1-D data structures\n",
    "2. No need to apply loops and if for conditions\n",
    "3. uses [] to access elements by indexes\n",
    "4. indexes can be numbers or text\n",
    "\n",
    "## pandas.DataFrame\n",
    "1. 2-D data structures\n",
    "2. combo of multiple series\n",
    "3. Equiv to regular tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "l1 = [12,34,56,32,12,3,4,57,54,46,56,78,99,100,23]\n",
    "print(l1)\n",
    "\n",
    "ser1 = pd.Series(l1)    # ,name='series_name'\n",
    "print(ser1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12, 34, 56, 32, 12, 3, 4, 57, 54, 46, 56, 78, 99, 100, 23]\n",
      "0      12\n",
      "1      34\n",
      "2      56\n",
      "3      32\n",
      "4      12\n",
      "5       3\n",
      "6       4\n",
      "7      57\n",
      "8      54\n",
      "9      46\n",
      "10     56\n",
      "11     78\n",
      "12     99\n",
      "13    100\n",
      "14     23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# for pd.Series, indexes can also be user defined\n",
    "ser2 = pd.Series(l1, index= range(1,len(l1)+1))\n",
    "# index takes a list as input - \n",
    "#        i. elements should be unique\n",
    "#        ii. the size of the list should be same as the size of the series\n",
    "print(ser2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1      12\n",
      "2      34\n",
      "3      56\n",
      "4      32\n",
      "5      12\n",
      "6       3\n",
      "7       4\n",
      "8      57\n",
      "9      54\n",
      "10     46\n",
      "11     56\n",
      "12     78\n",
      "13     99\n",
      "14    100\n",
      "15     23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "ser3 = pd.Series(l1, index = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 0, 'j', 23, 2, 'word_idx', 'z', 1])\n",
    "print(ser3)\n",
    "ser3[1] # this gives element corresponding to user defined function"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a            12\n",
      "b            34\n",
      "c            56\n",
      "d            32\n",
      "e            12\n",
      "f             3\n",
      "g             4\n",
      "h            57\n",
      "0            54\n",
      "j            46\n",
      "23           56\n",
      "2            78\n",
      "word_idx     99\n",
      "z           100\n",
      "1            23\n",
      "dtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "index to series can be \n",
    "\n",
    "1. Default (.iloc[]) 0 to n-1. It stays forever internally. If user doesnt define its own index, then both int and ext idx are from 0 to n-1, n:number of elements in series\n",
    "2. User defined (.loc[]). Ext index is same as of user defined. Int idx is 0 to n-1. loc supports internal index but if ext idx is defined, then loc wont support int idx. Hence, if ext idx is defined, loc is preffered for both row and col fetching"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "ser4 = pd.Series(l1, index=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o'])\n",
    "ser4.loc['a']\n",
    "# ser4.loc[0]       it wont work"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(l1[3])\n",
    "# ser2\n",
    "# access fourth element - element with def index as 3\n",
    "print(ser3.iloc[3], ser3.loc[0], ser3[1])\n",
    "print(ser3['word_idx'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32\n",
      "32 54 23\n",
      "99\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(ser1.iloc[0:8])   # rhl will be omitted. ser1 dont have ext idx\n",
    "print(ser2.iloc[0:8])   # rhl will be omitted. ser2 have ext idx\n",
    "# both will give seven rows, but the internal indexing is same in both series. On printing, ext idx overrides to display in ser2."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    12\n",
      "1    34\n",
      "2    56\n",
      "3    32\n",
      "4    12\n",
      "5     3\n",
      "6     4\n",
      "7    57\n",
      "dtype: int64\n",
      "1    12\n",
      "2    34\n",
      "3    56\n",
      "4    32\n",
      "5    12\n",
      "6     3\n",
      "7     4\n",
      "8    57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(ser2.iloc[0:80])  # only int idx which are valid within range wll be displayed without errors"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1      12\n",
      "2      34\n",
      "3      56\n",
      "4      32\n",
      "5      12\n",
      "6       3\n",
      "7       4\n",
      "8      57\n",
      "9      54\n",
      "10     46\n",
      "11     56\n",
      "12     78\n",
      "13     99\n",
      "14    100\n",
      "15     23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(ser2.loc[1:9]) # Contrary to the regular 1,2,3,4,5,6,7,8 a colon op in .loc will get 1,2,3,4,5,6,7,8,9\n",
    "print(ser2.loc[0:90])  # only the valid row with ext idx within the range will be displayed without throwing any errors"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1    12\n",
      "2    34\n",
      "3    56\n",
      "4    32\n",
      "5    12\n",
      "6     3\n",
      "7     4\n",
      "8    57\n",
      "9    54\n",
      "dtype: int64\n",
      "1      12\n",
      "2      34\n",
      "3      56\n",
      "4      32\n",
      "5      12\n",
      "6       3\n",
      "7       4\n",
      "8      57\n",
      "9      54\n",
      "10     46\n",
      "11     56\n",
      "12     78\n",
      "13     99\n",
      "14    100\n",
      "15     23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying conditions to series"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ser1 > 30"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     False\n",
       "1      True\n",
       "2      True\n",
       "3      True\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7      True\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11     True\n",
       "12     True\n",
       "13     True\n",
       "14    False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "ser1[ser1 > 30]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1      34\n",
       "2      56\n",
       "3      32\n",
       "7      57\n",
       "8      54\n",
       "9      46\n",
       "10     56\n",
       "11     78\n",
       "12     99\n",
       "13    100\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ser1[(ser1 > 30) & (ser1 < 80)] # comparision happens on series\n",
    "\n",
    "#and &\n",
    "#or  |\n",
    "#not ~"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1     34\n",
       "2     56\n",
       "3     32\n",
       "7     57\n",
       "8     54\n",
       "9     46\n",
       "10    56\n",
       "11    78\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "ser1.loc[(ser1 > 30) & (ser1 < 80)]\n",
    "# mentioning .loc is the conventional way of applying the conditions, because loc can take boolean values. iloc cant take boolean values."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1     34\n",
       "2     56\n",
       "3     32\n",
       "7     57\n",
       "8     54\n",
       "9     46\n",
       "10    56\n",
       "11    78\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "ser1[[1,3,6]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    34\n",
       "3    32\n",
       "6     4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions associated with series"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(dir(ser1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['T', '_AXIS_LEN', '_AXIS_ORDERS', '_AXIS_REVERSED', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_by_level', '_agg_examples_doc', '_agg_see_also_doc', '_align_frame', '_align_series', '_arith_method', '_attrs', '_binop', '_builtin_table', '_can_hold_na', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_expanddim', '_constructor_sliced', '_convert', '_convert_dtypes', '_cython_table', '_data', '_dir_additions', '_dir_deletions', '_drop_axis', '_drop_labels_or_levels', '_find_valid_index', '_flags', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cacher', '_get_cleaned_column_resolvers', '_get_cython_func', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_get_values', '_get_values_tuple', '_get_with', '_gotitem', '_hidden_attrs', '_index', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_init_dict', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_builtin_func', '_is_cached', '_is_copy', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_item_cache', '_ix', '_ixs', '_logical_func', '_logical_method', '_map_values', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_mgr', '_min_count_stat_function', '_name', '_needs_reindex_multi', '_obj_with_exclusions', '_protect_consolidate', '_reduce', '_reindex_axes', '_reindex_indexer', '_reindex_multi', '_reindex_with_indexers', '_replace_single', '_repr_data_resource_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_selected_obj', '_selection', '_selection_list', '_selection_name', '_set_as_cached', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_labels', '_set_name', '_set_value', '_set_values', '_set_with', '_set_with_engine', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take_with_is_copy', '_to_dict_of_blocks', '_try_aggregate_string_function', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'argmax', 'argmin', 'argsort', 'array', 'asfreq', 'asof', 'astype', 'at', 'at_time', 'attrs', 'autocorr', 'axes', 'backfill', 'between', 'between_time', 'bfill', 'bool', 'clip', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'divmod', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtype', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'ewm', 'expanding', 'explode', 'factorize', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'ge', 'get', 'groupby', 'gt', 'hasnans', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'interpolate', 'is_monotonic', 'is_monotonic_decreasing', 'is_monotonic_increasing', 'is_unique', 'isin', 'isna', 'isnull', 'item', 'items', 'iteritems', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'mad', 'map', 'mask', 'max', 'mean', 'median', 'memory_usage', 'min', 'mod', 'mode', 'mul', 'multiply', 'name', 'nbytes', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'radd', 'rank', 'ravel', 'rdiv', 'rdivmod', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'repeat', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'searchsorted', 'sem', 'set_axis', 'set_flags', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'squeeze', 'std', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_frame', 'to_hdf', 'to_json', 'to_latex', 'to_list', 'to_markdown', 'to_numpy', 'to_period', 'to_pickle', 'to_sql', 'to_string', 'to_timestamp', 'to_xarray', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unique', 'unstack', 'update', 'value_counts', 'values', 'var', 'view', 'where', 'xs']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ser1.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "ser1.mean() "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "44.4"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "ser1.median()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ser1.mode() # always gives a dataframe, even if only one mode is possible"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    12\n",
       "1    56\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "ser1.min()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "ser1.max()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "ser1.quantile([0,0.25,0.5,0.75,0.65,1]) # gives percentile values, not just limited to quantiles and min&max"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.00      3.0\n",
       "0.25     17.5\n",
       "0.50     46.0\n",
       "0.75     56.5\n",
       "0.65     56.0\n",
       "1.00    100.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "ser1.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count     15.000000\n",
       "mean      44.400000\n",
       "std       31.497846\n",
       "min        3.000000\n",
       "25%       17.500000\n",
       "50%       46.000000\n",
       "75%       56.500000\n",
       "max      100.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# DF|Ser.isnull() or .notnull() returns df|ser of True or False\n",
    "\n",
    "# .to_numeric works only for list, tuple, 1-d array, or Series dtype\n",
    "#  pd.to_numeric(ser1 [, errors='ignore'(i/p)|'raise'(exception)|'coerce'(NaN)] [, downcast=None|'integer'|'signed'|'float'|'unsigned'])        any data type to numbers\n",
    "#  pd.to_datetime()       suitable characters to date type\n",
    "# --------------------------------------------------------------\n",
    "# astype dont work on columns of mixed type. It truncates the decimal part if float->int\n",
    "# dataframe.astype(str)   anything(DF or S or DF.S.....) to string (object type)\n",
    "# dataframe.astype(bool)  anything(DF or S or DF.col...) to bool ([u]int8,16,32,64(d); float16,32,64(d),128, and boolean)\n",
    "# shortcut\n",
    "# df = df.astype({'string_col': 'float16','int_col': 'float16'})\n",
    "\n",
    "# eg to demonstrate to replace numpy's NaN(float64) to pandas NA, to replace it with any value\n",
    "# df['mix_col'] = pd.to_numeric(df['mix_col'], errors='coerce').astype('Int64')\n",
    "# df['mix_col'] = pd.to_numeric(df['mix_col'], errors='coerce').fillna(0).astype('int')\n",
    "\n",
    "#.nunique()\n",
    "#.astype('category')\n",
    "\n",
    "#def convert_money(value):\n",
    "#       value = value.replace('£','').replace(',', '')\n",
    "#       return float(value)>>>\n",
    "# df['money_col'].apply(convert_money)\n",
    "# OR df['money_col'].apply(lambda v: v.replace('£','').replace(',' , '')).astype('float')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import numpy as np\n",
    "\n",
    "df90 = pd.DataFrame(np.random.random((3,2)), columns =[\"A\", \"B\"])\n",
    "print(df90.round(decimals=2))\n",
    "df90 = df90*10\n",
    "print(df90)\n",
    "print(pd.to_numeric(df90['A'], downcast='float').round().astype('int8'))\n",
    "df90>3\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      A     B\n",
      "0  0.73  0.62\n",
      "1  0.03  0.30\n",
      "2  0.16  0.91\n",
      "          A         B\n",
      "0  7.309596  6.236959\n",
      "1  0.337610  3.026212\n",
      "2  1.625162  9.115406\n",
      "0    7\n",
      "1    0\n",
      "2    2\n",
      "Name: A, dtype: int8\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       A     B\n",
       "0   True  True\n",
       "1  False  True\n",
       "2  False  True"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "ser1.astype(int)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      12\n",
       "1      34\n",
       "2      56\n",
       "3      32\n",
       "4      12\n",
       "5       3\n",
       "6       4\n",
       "7      57\n",
       "8      54\n",
       "9      46\n",
       "10     56\n",
       "11     78\n",
       "12     99\n",
       "13    100\n",
       "14     23\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "ser1.astype(float)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      12.0\n",
       "1      34.0\n",
       "2      56.0\n",
       "3      32.0\n",
       "4      12.0\n",
       "5       3.0\n",
       "6       4.0\n",
       "7      57.0\n",
       "8      54.0\n",
       "9      46.0\n",
       "10     56.0\n",
       "11     78.0\n",
       "12     99.0\n",
       "13    100.0\n",
       "14     23.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "ser1.astype(object)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      12\n",
       "1      34\n",
       "2      56\n",
       "3      32\n",
       "4      12\n",
       "5       3\n",
       "6       4\n",
       "7      57\n",
       "8      54\n",
       "9      46\n",
       "10     56\n",
       "11     78\n",
       "12     99\n",
       "13    100\n",
       "14     23\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "ser1.clip?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mser1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool_t'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'FrameOrSeries'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Trim values at input threshold(s).\n",
      "\n",
      "Assigns values outside boundary to boundary values. Thresholds\n",
      "can be singular values or array like, and in the latter case\n",
      "the clipping is performed element-wise in the specified axis.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "lower : float or array_like, default None\n",
      "    Minimum threshold value. All values below this\n",
      "    threshold will be set to it.\n",
      "upper : float or array_like, default None\n",
      "    Maximum threshold value. All values above this\n",
      "    threshold will be set to it.\n",
      "axis : int or str axis name, optional\n",
      "    Align object with lower and upper along the given axis.\n",
      "inplace : bool, default False\n",
      "    Whether to perform the operation in place on the data.\n",
      "*args, **kwargs\n",
      "    Additional keywords have no effect but might be accepted\n",
      "    for compatibility with numpy.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "Series or DataFrame or None\n",
      "    Same type as calling object with the values outside the\n",
      "    clip boundaries replaced or None if ``inplace=True``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "Series.clip : Trim values at input threshold in series.\n",
      "DataFrame.clip : Trim values at input threshold in dataframe.\n",
      "numpy.clip : Clip (limit) the values in an array.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n",
      ">>> df = pd.DataFrame(data)\n",
      ">>> df\n",
      "   col_0  col_1\n",
      "0      9     -2\n",
      "1     -3     -7\n",
      "2      0      6\n",
      "3     -1      8\n",
      "4      5     -5\n",
      "\n",
      "Clips per column using lower and upper thresholds:\n",
      "\n",
      ">>> df.clip(-4, 6)\n",
      "   col_0  col_1\n",
      "0      6     -2\n",
      "1     -3     -4\n",
      "2      0      6\n",
      "3     -1      6\n",
      "4      5     -4\n",
      "\n",
      "Clips using specific lower and upper thresholds per column element:\n",
      "\n",
      ">>> t = pd.Series([2, -4, -1, 6, 3])\n",
      ">>> t\n",
      "0    2\n",
      "1   -4\n",
      "2   -1\n",
      "3    6\n",
      "4    3\n",
      "dtype: int64\n",
      "\n",
      ">>> df.clip(t, t + 4, axis=0)\n",
      "   col_0  col_1\n",
      "0      6      2\n",
      "1     -3     -4\n",
      "2      0      3\n",
      "3      6      8\n",
      "4      5      3\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "df = pd.Series([4,4,4,6,8,4,10,15,6,15,15])\n",
    "df.duplicated()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     False\n",
       "1      True\n",
       "2      True\n",
       "3     False\n",
       "4     False\n",
       "5      True\n",
       "6     False\n",
       "7     False\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# 5 rows max by default\n",
    "print(ser1.head(3))\n",
    "print(ser1.tail(3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    12\n",
      "1    34\n",
      "2    56\n",
      "dtype: int64\n",
      "12     99\n",
      "13    100\n",
      "14     23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(ser1.index)\n",
    "print(ser3.index)\n",
    "print(ser1.index.tolist())\n",
    "print(ser3.index.tolist())\n",
    "print(ser3.tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RangeIndex(start=0, stop=15, step=1)\n",
      "Index(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 0, 'j', 23, 2, 'word_idx', 'z',\n",
      "       1],\n",
      "      dtype='object')\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 0, 'j', 23, 2, 'word_idx', 'z', 1]\n",
      "[12, 34, 56, 32, 12, 3, 4, 57, 54, 46, 56, 78, 99, 100, 23]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "ser3.to_csv(\"ser3.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df = pd.read_csv('./sosurveydataset/survey_results_public.csv')\n",
    "# here we can write second param as <index_col='Respondent'> as it is eligible for being indexed. \n",
    "# we can also provide normally ext idx as <index=[..,..,..,..,......]>\n",
    "#\n",
    "#df = pd.read_csv('dataset.csv', dtype={'string_col': 'float16','int_col': 'float16'})\n",
    "\n",
    "# headers=None|rownum   (tells reader csv reader to make column header rownum number, or none in case of no headers are present)\n",
    "# skiprows=num|[list of rownums]|lambda idx: idx%3==0      (skips num rows or rows from list from starting)\n",
    "# skipfooter=num|\"\n",
    "\n",
    "# If headers are not present, and we want to give custom headers, names=[list of headers]\n",
    "\n",
    "# nrows=num     (makes us to read only first num rows into DF, excluding headers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_columns', 12)\n",
    "pd.set_option('display.max_rows', 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()\n",
    "# 1. Row - 32 entries with 0 - 31 and Data columns (total 15 columns)\n",
    "# 2. All columns - followed by it's data types\n",
    "        # object - strings\n",
    "        # number of non missing values in that column\n",
    "# 3. Freq table of the data types"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.Respondent.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.columns.tolist() # there is nothing like df.rows method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.describe()   # cols only with numerical values will be taken into cinsideration. Rows with NaN will be excluded."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for dataframes when used with loc or iloc, rows and cols both are required inside []\n",
    "# For using loc and iloc, it's nesessary to pass both column and row\n",
    "# [r,c] -> left of comma - row indexes/conditions and right is for column names\n",
    "\n",
    "df.loc[0:15,['Respondent', 'Age', 'CompTotal']] # except idx, everything is compulsory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Try to extract columns\n",
    "\n",
    "# 1. Use of the . operator\n",
    "print(df.YearsCode)\n",
    "print(df['YearsCode'])  # prefer [] over . because if col named count, it will overridden by the method count\n",
    "print(df[['Respondent', 'Age', 'CompTotal']])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Age'] = pd.to_numeric(df.Age)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['CompTotal'] = df.CompTotal.astype(bool)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.CompTotal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.Respondent = df['Respondent'].astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Corey's"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "people = {\n",
    "    'first' : ['Corey', 'Jane', 'John'],\n",
    "    'Last' : ['Schafer', 'Doe', 'Doe'],\n",
    "    'email' : ['CoreySchafer@gmail.com', 'JaneDoe@gmail.com', 'JohnDoe@gmail.com']\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.DataFrame(people)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.shape    # gives dimension. NOTE, its w/o brackets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['first'].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['first'].size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# index to dataframe\n",
    "df1.loc[0] # or df.iloc[0], both will give same output. Here, for single row, column idx is idx of the output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for multiple rows if to be fetched, then normal output is observed\n",
    "\n",
    "df1.iloc[[1,2], 0]   # from \" , 0\" is optional. For multiple rows/cols [] is required, but for single row/col only that row's/col's idx is required...  Works both for iloc and loc, but remember that loc wont take int idx if ext idx is defined. Here col headers are ext idx which are implicitly defined.\n",
    "# Syntax:       .[i]loc[[row[,row,row,...]][,[col[,col,col,...]] ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# WORKS BUT IRRELEVANT\n",
    "df1.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Hobbyist'].value_counts()   # gives the frequency of the unique values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# making the index col by one series of dataframe\n",
    "\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "df1.set_index('email')\n",
    "print(df1)   # inplace change didn't take place\n",
    "print(df1.index)\n",
    "df1.set_index('email', inplace=True)\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "df1.reset_index(inplace=True)\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "\n",
    "# one more application: if we make new df of filtered DF the index are having holes. reset_index([drop=True][, inplace=True]) will make ne contiguous index, w/ making old hollow index as additional series.\n",
    "# drop=True will drop that old hollow index to be appended as a series to the filtered DF"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sorting rows based on index\n",
    "df2=pd.read_csv('./sosurveydataset/survey_results_schema.csv', index_col='Column')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2.sort_index()    # this didn't take inplace change. For that, use <inplace=True> as parameter to the sort_index() method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.loc[df['ConvertedComp'] > 70000, ['ConvertedComp', 'Country']]    # comparision happens on series"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries = ['United States', 'India', 'United Kingdom', 'Germany', 'Canada']\n",
    "filt = df['Country'].isin(countries)\n",
    "df.loc[filt]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filt = df['LanguageWorkedWith'].str.contains('Python', na=False)    # ,case=True (makes case-sensitive)  ,regex=True(parse pattern as regex)\n",
    "df.loc[filt, 'LanguageWorkedWith']\n",
    "\n",
    "# import re\n",
    "# df.loc[df['LanguageWorkedWith'].str.contains('fire|grass', flags=re.I, regex=True, na=False)]   # '^pi[a-z]*'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating column names of DFs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# changing all columns name inplace by default\n",
    "print(df1)\n",
    "df1.columns = ['Email', 'First Name', 'Last Name'] # this methos requires all column of DFs, even if all name changes are not required\n",
    "print(df1.columns)\n",
    "df1.columns = [x.upper() for x in df1.columns]\n",
    "print(df1.columns)\n",
    "df1.columns = df1.columns.str.replace(' ','-')\n",
    "print(df1.columns)\n",
    "df1.columns = [x.lower().replace('-','_') for x in df1.columns]\n",
    "print(df1.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## df.replace(single_val|list_of_vals, replacing_val|list_of_vals)      (corrsponding values will get repalced by the corresponding values)\n",
    "## df.replace({'col1': single_val|list_of_vals,   'col2': single_val|list_of_vals, ... }, np.NaN)\n",
    "## df.replace({'val1' : replacing_val,  'val2' : replacing_val, ...})\n",
    "## df.replace({'col1' : '[A-Za-z]',   'col2' : 'some_othr_pat', ... }, replacing_val, regex=True)\n",
    "# (regex will not replace the whole cell value with the new replacing vale. INstead it will just replace the part of the found matched string. Rest part of the string will be untouched)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# changing specific column name with explicitly mentioning inplace parameter\n",
    "df1.rename(columns={'first_name':'first', 'last_name':'last'}, inplace=True)\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating data in DFs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# changing single row and all of its column vaue, i.e., entirely whole row\n",
    "df1.loc[2] = ['johnsmith@gmail.com', 'John', 'Smith']\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# changing specific columns for a given row\n",
    "df1.loc[1, ['email', 'last']] = ['JaneDoodle@email.com', 'Doodle']  # Also, for only single vale to be change, list is not required in LHS or RHS\n",
    "df1\n",
    "# one can also use df1.[filt, col(s)] = <[>col(s)<]>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['email'] = df1['email'].str.lower()\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### apply\n",
    "used for calling function on every values of our data structure. Can be applied on Series as well as on DFs. NOT inplace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.email.apply(len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.apply(len, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.size #r*c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def update_email(email):\n",
    "    return email.upper()\n",
    "\n",
    "df1['email'] = df1.email.apply(update_email)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['email'] = df1.email.apply(lambda x : x.lower())\n",
    "print(df1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# apply on dataframes\n",
    "print(df1.apply(len))   # axis is set to rows by default\n",
    "print(df1.apply(len, axis = 'columns'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding minimum value from each series of the dataframe\n",
    "df1.apply(pd.Series.min)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# while lambda function runs on the Series by default. We dont need to explicitly specify the pd.seried while using the dataframe with apply\n",
    "df1.apply(lambda x : x.min())   # look at it. X is itself a series, and we are finding the minimum from every series. This is useful for numerical analysis."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### applymap\n",
    "It works only on the dataframes. It does not work on the series data structures. It is used to run functions on every element of the dataframes. NOT inplace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df1.applymap(len))\n",
    "print(df1.applymap(str.lower))\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### map\n",
    "It only works on a series. It is used to substitute each value in the series with another value. NOT inplace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['first'].map({'Corey':'kirrr', 'Jane':'girrrh'})    # those values which are not mentioned in the dictionary, will be turned into NaN.\n",
    "# for permanent change, use same series in LHS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### replace\n",
    "for handling the error of the map that gives NaN, we use replace. Here, only specfied values will get altered. NOT inplace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['first'].replace({'Corey':'kirrr', 'Jane':'girrrh'})\n",
    "# for permanent change, use same series in LHS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this changes the value based on certain conditions\n",
    "# df.loc[df['Type 1'] == 'Fire', 'Legendary'] = '<some_val>'\n",
    "\n",
    "# below, if rhs is single value w/o(preffered for single value) or w/ list, then both, or cols in the LHS will be set to the value of the RHS, else correspondingly\n",
    "# df.loc[df['Type 1'] == 'Fire', ['Legendary1', 'Leg12']] = ['<some_val1>', '<some val2>']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add columns\n",
    "df1['full_name'] = df1['first'] + ' ' + df1['last']\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# removing cols\n",
    "df1.drop(columns='full_name', inplace=True)   # or, columns=['col1', 'col2', ...]\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['full_name'] = df1['first'] + ' ' + df1['last']\n",
    "\n",
    "# split column\n",
    "\n",
    "df1[['firstname', 'lastname']] = df1['full_name'].str.split(' ',expand = True)\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### removing or adding rows of data\n",
    "\n",
    "append doesn't have inplace parameter. Hence for permanent change, use df1= df1.appen......................"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# adding row of data. This is NOT INPLACE change\n",
    "df1.append({'firstname':'sharan'}, ignore_index=True)\n",
    "df1.append({'firstname':['wubba', 'lubba', 'dub', 'dubb']}, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppl = {\n",
    "    'first' : ['Tony', 'Steve'],\n",
    "    'last' : ['Stark', 'Rogers'],\n",
    "    'email' : ['notyourkakkar@avengers.com', 'itsyourcap@avengers.com']\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ndf = pd.DataFrame(ppl)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = df1.append(ndf, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# removing rows\n",
    "df1.drop(index=4)   # NOT INPLACE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filt = df1['last'] == 'Doodle'\n",
    "print(filt)\n",
    "df1.drop(index=df1[filt].index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sorting data in pandas\n",
    "\n",
    "pass parameter as inplace=True for permanent shufflibg of value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.sort_values(by = 'last')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.sort_values(by = ['last', 'first'], ascending = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.sort_values(by = ['first', 'last'], ascending = [False, True])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sorting by index. It is used to restore the DF in the original order of rows as per the internal index\n",
    "df1.sort_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#viewing just sorted series of a dataframe\n",
    "df1['email'].sort_values()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# displaying Nth largest values...\n",
    "# M1: df1[[col1, col2,....]].head(n)    where the dataframe is sorted\n",
    "# M2: below\n",
    "df['ConvertedComp'].nlargest(10)    # also there is nsmallest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.nlargest(10, 'ConvertedComp')    # also there is nsmallest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Country'].value_counts(normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.loc[df['Country'] == 'India', ['CompFreq']].value_counts(normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_grp = df.groupby(['Country'])\n",
    "country_grp.get_group('India')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "country_grp['CompFreq'].value_counts().head(50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_grp['CompFreq'].value_counts().loc['India']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_grp['CompTotal', 'Respondent'].agg(['mean', 'median'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filt = df['Country'] == 'India'\n",
    "df.loc[filt]['LanguageWorkedWith'].str.contains('Python', na=False).sum() # sum works on boolean also, takes True as 1 and False as 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_grp['LanguageWorkedWith'].str.contains('Python', na=False).sum()    # ERROR since it is group by object, not a normal object. Hence we will use the apply method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_grp['LanguageWorkedWith'].apply(lambda x : x.str.contains('Python', na = False).sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_respondents = df.loc[:,'Country'].value_counts()\n",
    "country_respondents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "country_uses_python = country_grp['LanguageWorkedWith'].apply(lambda x : x.str.contains('Python', na=False).sum())\n",
    "country_uses_python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python_df = pd.concat([country_respondents, country_uses_python], axis = 'columns')\n",
    "python_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## keys=['df1_alias', 'df2_alias']      ( this will make super index irrespective of the exixsting index, BUT WILL ONLY WORK WHEN axis-'index')\n",
    "\n",
    "## It might be possible during axis='columns' that the nnumber of rows are not equal. This will lead to mismatching of the corresponding index order\n",
    "# this will lead to creation of new index range in final output, but the original DFs indexes will get converted to series in O/P DF\n",
    "# to avoid this, create index=[0,1,2,...] and index=[4,2,0,1,3,...] in corresponding DFs\n",
    "# therefore, while concatenating, the indexes will be correspondingly wqual, but the before cstom index will be now series, but in order\n",
    "# later, drop one of the repeating series\n",
    "\n",
    "## series can also be cancatenated."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# merge dataframes : Its like join in SQL. Unlike concat, its not concatenating on the index column. But, it is merging on the column, even if they are jumbled\n",
    "# Only common columns will be in the new DF, by default\n",
    "df33=pd.DataFrame({\n",
    "    'city' : ['NY', 'Chic', 'Orlando'],\n",
    "    'Temp' : [21,14,35]\n",
    "})\n",
    "df33"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df44 = pd.DataFrame({\n",
    "    'city' : ['Chic', 'NY', 'Orlando'],\n",
    "    'humidity' : [65,68,75]\n",
    "})\n",
    "df44"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df55=pd.merge(df33, df44, on='city')        # how='inner'|'outer'|'left'|''right'\n",
    "df55"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in case where data series in DF are having same exact name for [some] columns, then pandas imlicitly appens _x & _y. ==> suffixex=('_left', '_right')\n",
    "# indicator=True    (will make additional column named '_merge' having values 'left_only', 'right_only', 'both')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python_df.rename(columns={'Country':'NumRespondents', 'LanguageWorkedWith':'NumKnowsPython'}, inplace=True)\n",
    "python_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python_df['PctKnowsPython'] = (python_df['NumKnowsPython'] / python_df['NumRespondents']) * 100\n",
    "python_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python_df.sort_values(by = 'PctKnowsPython', ascending=False, inplace=True)\n",
    "python_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python_df.loc['India']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PIVOT FUNCTION is just used for restructuring the dataframe. combination of Index should be unique per columns for the new dataframe pivot\n",
    "df77 = pd.read_csv('./sosurveydataset/wheather.csv')\n",
    "df77"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df77.pivot(index='date', columns='city')    # ,values='humidity'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pivot table allows to summarize and aggregate the tabular data. Suppose we want to aggregate the data\n",
    "df77 = pd.read_csv('./sosurveydataset/wheather2.csv')\n",
    "\n",
    "df77.pivot_table(index='city', columns='date', margins=True)  # mean by default # margins is optional\n",
    "# aggfunc='sum'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df77 = pd.read_csv('./sosurveydataset/wheather2.csv')\n",
    "df77['date'] = pd.to_datetime(df77['date'])\n",
    "df77.pivot_table(index=pd.Grouper(freq='M', key='date'), columns='city')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# melt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df69 = pd.DataFrame({\n",
    "    'day' : ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    'chicago' : [32,30,28,22,30,20,25],\n",
    "    'chennai' : [75,77,75,82,83,81,77],\n",
    "    'berlin' : [41,43,45,38,30,45,47]\n",
    "})\n",
    "\n",
    "df69"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.melt(df69, id_vars=['day'])  # id+vars are the cols that we want on x-axis, basically that we want keep intact\n",
    "# var_name='col_name instead of variable'\n",
    "# val_name='col_name instead of value'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reshaping DataFrame using Stack/Unstack\n",
    "df96 = pd.read_excel(\"stocks.xlsx\",header=[0,1])\n",
    "df96"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df96.stack()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df96.stack(level=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df96_stacked=df96.stack()\n",
    "df96_stacked"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df96_stacked.unstack()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3 levels of column headers\n",
    "df67 = pd.read_excel(\"stocks_3_levels.xlsx\",header=[0,1,2])\n",
    "df67"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df67.stack()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df67.stack(level=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df67.stack(level=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# contingency table | crosstab | cross table\n",
    "\n",
    "df007 = pd.read_excel('./survey.xls')\n",
    "df007"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.crosstab(df007.Nationality, df007.Handedness)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.crosstab(df007.Sex, df007.Handedness)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.crosstab(df007.Nationality, df007.Handedness, margins=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.crosstab(df007.Nationality, [df007.Handedness, df007.Sex])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "pd.crosstab([df007.Nationality, df007.Sex], df007.Handedness)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.crosstab(df007.Nationality, df007.Handedness, margins='index')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "pd.crosstab(df007.Sex, df007.Handedness, values=df007.Age, aggfunc=np.average)  # avg age of combinations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values\n",
    "It is not inplace change. For that to happen, use 'inplace' argument and set it to True"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "people = {\n",
    "    'first': ['Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'], \n",
    "    'last': ['Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'], \n",
    "    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],\n",
    "    'age': ['33', '55', '63', '36', None, None, 'Missing']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(people)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dropna() # rows containing missing values are removed\n",
    "# .dropna(thresh=2_OrAnyNum) (implies that if the row has at least 2 valid values, then keep it. Otherwise drop it. NA values are non-valid values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dropna(axis='index', how='any')  # they are default parameters; axis decides whether to drop that row or that columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dropna(how='all')    # drop rows only if all the values in that row is empty"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fetching rows that dont have empty email address\n",
    "df.dropna(axis='index', subset=['email'])   # how is not relevant because only email column is been looked for empty values. all or any wont create any difference\n",
    "# subset argument will take column names that will be cheked for missing values; even though single item is passed, but it needs to be in the list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fetching rows that have at least either of lastname or email\n",
    "df.dropna(axis='index', how='all', subset=['last', 'email'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "handling custom empty value. eg, here 'NA' or 'Missing' are empty for us, but for python, its just an object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.replace('NA', np.nan, inplace=True)\n",
    "df.replace('Missing', np.nan, inplace=True)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.isna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "filling na values with the particular values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.fillna('Missing')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# .fillna({ 'col1': list_or_single_na_considered_value(s), 'col2': list_or_single_na_considered_value(s) , ... })\n",
    "\n",
    "# .fillna(method='ffill' [, axis=1|0] [, limit=1(or any number)])       # fills data of immediate above cell\n",
    "# bfill"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## INTERPOLATION\n",
    "\n",
    "# df.interpolate()      (fills the avg data)\n",
    "\n",
    "# df.interpolate(method=time)       ( I think it works when the index is datetime column. So instead of median, it just smartly checks for the date based value )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['age'].mean()    # this will give error because 'age' col is of type object/string. mean() doesn't work on the age columns. So we need to convert to numbers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(np.nan)    # np.nan is of type float"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if we try to convert the column containing nan(float) to integer type, then this will give an error. If col wouldn't have missing values, then it would have just worked fine\n",
    "df['age'] = df['age'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# so we need to either replace missing values by 0, which will alter the actual average value of that col\n",
    "# else we have to cast this col values to float\n",
    "df['age'] = df['age'].astype(float)\n",
    "df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['age'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# one of the many ways to add total columns. Observe that here .sum() is working on the row, instead of its default behaviour, ie on the whole series\n",
    "# df['Total'] = df.iloc[:, 4:6].sum(axis='rows')\n",
    "\n",
    "# REORDERING OF THE COLS\n",
    "# cols = df.columns.tolist\n",
    "# df = df[cols[0:4] + [cols[-1]] + cols[4:12]]  we can also use names of the columns instead of hard coded numbers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "na_vals = ['NA', 'Missing']\n",
    "df = pd.read_csv('./sosurveydataset/survey_results_public.csv', na_values=na_vals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for specififc cols, what na_values may be defined for whole DF, might be actual data for some underlying series, e.g., -1\n",
    "\n",
    "So, for that we can construct na_vals as dict as\n",
    "na_vals= { 'col1':['missing', 'n.a.'], 'col2':['missing', -1, 'n.a.'] }\n",
    "Here, it maybe possible taht 'col1' have -1 as actual valid data. Hence, we specified column wise na_values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['YearsCode'].head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# since col has some NaN value, hence it is of object type\n",
    "# it also has some string values. So, even after converting to float, mean() wont work because some actual strings are present\n",
    "df['YearsCode'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['YearsCode'].replace('Less than 1 year', 0, inplace=True)\n",
    "df['YearsCode'].replace('More than 50 years', 51, inplace=True)\n",
    "df['YearsCode'].unique()    # dtype is still an object. Hence we need to convert it to the float"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['YearsCode'] = df['YearsCode'].astype(float)\n",
    "df['YearsCode'].dtype"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['YearsCode'].mean()  # now we got the answer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datetime series analysis\n",
    "https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# date\n",
    "# day:          %d\n",
    "# dayofweek(0-6)%w\n",
    "# weekdayabbr   %a\n",
    "# weekdayfulln  %A\n",
    "# monthnum      %m\n",
    "# monthnameabbr %b\n",
    "# monthnamefull %B\n",
    "# yearwocent    %y\n",
    "# yearwcent     %Y\n",
    "# Hour(00-24)   %H\n",
    "# Hour(00-12)   %I\n",
    "# Minutes       %M\n",
    "# Seconds       %S\n",
    "# fracsex       %f\n",
    "# am/pm         %p"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date1 = \"25Feb2018\"\n",
    "date2 = \"1/1/18\"\n",
    "date3 = \"01/23/1986\"\n",
    "date4 = \"Sunday 25 Feb 2018\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.to_datetime(date1,format=\"%d%b%Y\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d = pd.to_datetime(date3,format=\"%m/%d/%Y\")\n",
    "print(d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./sosurveydataset/ETH_1h.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Date column is not even of datetime type. No datetime methods will work onto it\n",
    "df.loc[0, 'Date'].day_name()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d %I-%p')\n",
    "df.head() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.loc[0, 'Date'].day_name()    # running datetime method on single datetime value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# parsing datetime column at the time of read_csv\n",
    "\n",
    "d_parser = lambda x : pd.datetime.strptime(x, '%Y-%m-%d %I-%p')\n",
    "df1 = pd.read_csv('./sosurveydataset/ETH_1h.csv', parse_dates=['Date'], date_parser=d_parser)\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# running datetime methods on pandas series, we need to use dt class on the series object\n",
    "df1['Date'].dt.day_name()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['DayOfWeek'] = df1['Date'].dt.day_name()\n",
    "df1['DayOfWeek']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['Date'].min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['Date'].max() - df1['Date'].min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filt = (df1['Date'] >= '2019') & (df1['Date'] < '2020') # implicitly, pandas will know that it is year\n",
    "df1.loc[filt]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filt = (df1['Date'] >= pd.to_datetime('2019-01-01')) & (df1['Date'] < pd.to_datetime('2020-01-01'))\n",
    "df1.loc[filt]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.set_index('Date', inplace=True)\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# since Date is noe index, we can use it .loc[]\n",
    "df1['2019']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['2020-01':'2020-02']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['2020-01':'2020-02']['Close'].mean()\n",
    "# df1.loc['2020-01':'2020-02','Close'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1['2020-01':'2020-02']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# seeing max value per day. We need to resample our data\n",
    "highs = df1['High'].resample('D').max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "highs['2020-01-01']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "highs.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.resample('W').agg({'Close':'mean', 'High':'max', 'Low':'min', 'Volume':'sum'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## suppose the index id datetime column with holes/Gaps. To fill the DF with contiguous dates index rows:\n",
    "# dt = pd.date_range('01-01-2017', '01-11-2017')        (we made dates b/n two date ranges. It can be from min and max dates)\n",
    "# idx = pd.DatetimeIndex(dt)                            (we made those dates as index worthy)\n",
    "# df = df.reindex(idx)                                  (we reindexed the df with new date ranges. Overlaping wont get overwritten, just new rows will get added)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading & Writing data Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv('./pilot1.csv')\n",
    "# we can also pass the parameter index=False & header=False which makes sure that the index & header are not written to file.\n",
    "# This index=False param works with the any writing to file pandas function including in SQL\n",
    "# columns=['col1', 'col2',...] will make sure that only these columns will get written to the .csv file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tab-delimited files\n",
    "# df.to_csv('./pilot2.tsv', sep='\\t')\n",
    "# df = pd.read_csv('./pilot1.tsv', sep = '\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for wriring to Excel file, we need one package\n",
    "\n",
    "conda install xlwt(for older .xls) openpyxl(for new .xlsx) xlrd"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "## CONVERTING/REPLACING specific values in the excel data cell/Series into another value\n",
    "\n",
    "# def col1_func_n(cell):\n",
    "#       if cell=='some specific value':\n",
    "#           return another_val\n",
    "#       if cell=='other error value':\n",
    "#           return 'new replace_val'\n",
    "#\n",
    "# def col2_func_n(cell):\n",
    "#   blah blah blah blah\n",
    "\n",
    "# pd.read_excel('./pilot3.xlsx' [, \"Sheet1\"]  [, converters = {'col1':col1_func_n, 'col2':col2_func_n}])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df.to_excel('filename.xls[x]' [, sheet_name=\"stocks\"] [, startrow = rownum_starts_from_0] [, startcol = colnum_starts_from_0])\n",
    "\n",
    "## Also if we need to write two DFs to two sheets of same excel file\n",
    "# with pd.ExcelWriter('filename.xls') as writer:\n",
    "#       df1.to_excel(writer, sheet_name=\"sheet_name_1\")\n",
    "#       df2.to_excel(writer, sheet_name=\"sheet_name_2\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# to JSON; by default it makes it dict like\n",
    "# df.to_json('./pilot4.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we can change the to_json default param to make file in list like. I mean, one line for one record\n",
    "# df.to_json('./pilot4.json', orient='records', lines=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test = pd.read_json('./pilot4.json', orient='records', lines=True)\n",
    "# test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## connecting to SQL\n",
    "we need to install SQLAlchemy and psycopg2(for postgres SQL)\n",
    "\n",
    "It is popular ORM(Object Relational Mapper), which is just a way to use python objects in order to connect to database easily"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine('postgresql://dbuser:dbpass@localhost:5432/sample_db')\n",
    "\n",
    "india_df.to_sql('sample_table', engine) THIS WILL CREATE THE NON-EXISTING TABLE\n",
    "\n",
    "india_df.to_sql('sample_table', engine, if_exists='replace|append') THIS WILL CREATE THE EXISTING TABLE\n",
    "\n",
    "sql_df = pd.read_sql('sample_table', engine, index_col='Respondent')    IMPORTING TABLE DATA\n",
    "\n",
    "sql_df = pd.read_sql_query('SELECT * FROM sample_table', engine, index_col='Respondent')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://dbuser:@localhost:3306/application')\n",
    "\n",
    "df = pd.read_sql(\"tab_n\", engine [, columns=['col1', 'col2', ... ] ])\n",
    "\n",
    "===========================================================================================================================================================\n",
    "\n",
    "query = ''' sql query '''\n",
    "\n",
    "df = pd.read_sql_query(query, engine [ , chunksize= num] )\n",
    "\n",
    "===========================================================================================================================================================\n",
    "\n",
    "df <<== data populate dataframe with column name same as the table column names\n",
    "\n",
    "df.to_sql( name = 'tab_n', con=engine, index=False, if_exists='append'|'replace'|'fail')\n",
    "\n",
    "===========================================================================================================================================================\n",
    "\n",
    "query = ''' sql query '''\n",
    "\n",
    "df = pd.read_sql('tab_n'|query , engine)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading data from some online URL\n",
    "\n",
    "pst_df = pd.read_json('URL')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for df in pd.read_csv('./sosurveydataset/ETH_1h.csv', chunksize=5):\n",
    "#     print(\"Chunk:\")\n",
    "#     print(df)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}